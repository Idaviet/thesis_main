{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0b2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data from torch file. Unncessary if .npy files containing the IG is already done\n",
    "#import torch\n",
    "# Mason\n",
    "#pt = torch.load('mason_unique_results.pt', map_location='cpu').numpy()\n",
    "#labels = torch.load('mason_unique_label.pt', map_location='cpu').numpy()\n",
    "#sequ = torch.load('mason_unique_sequence.pt', map_location='cpu').numpy()\n",
    "\n",
    "# Brij\n",
    "#pt = torch.load('brij_short_results.pt', map_location='cpu').numpy()\n",
    "#labels = torch.load('brij_short_label.pt', map_location='cpu').numpy()\n",
    "#sequ = torch.load('brij_short_sequence.pt', map_location='cpu').numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8de26",
   "metadata": {},
   "source": [
    "pt = torch.load('mason_unique_results.pt', map_location='cpu').numpy()\n",
    "pt should be the same as your masonIG (probably a npy file and not a csv?)\n",
    "\n",
    "labels = torch.load('mason_unique_label.pt', map_location='cpu').numpy()\n",
    "labels should be a number with either 1 or 0, and should be in the mason_sequences_labeled.csv' file\n",
    "\n",
    "\n",
    "sequ = torch.load('mason_unique_sequence.pt', map_location='cpu').numpy()\n",
    "are the sequences them self and are also in the mason_sequences_labeled.csv' file. However in my code they are a one-hot encoded numpy tensor, while you already have the complete sequences. You dont need the one-hot encoding, so you need to remove the code that transforms the one-hot encoding into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c9b234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "#import logomaker #only to make a logo graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad50ab83",
   "metadata": {},
   "source": [
    "### Load Integrated Gradients (IG) labels and sequences into separate numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7140f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data from numpy files\n",
    "###Takes IG data from xyzIG.npy file and seq + label data from xyz_sequences_label.csv file\n",
    "\n",
    "\n",
    "#Mason\n",
    "ig_data =r\"C:\\Users\\isaac\\Desktop\\MTLS Masters Program\\Thesis\\Brijj-Mason_data\\masonIG.npy\"\n",
    "labeled_seq_data = r\"C:\\Users\\isaac\\Desktop\\MTLS Masters Program\\Thesis\\Brijj-Mason_data\\mason_sequences_label.csv\"\n",
    "\n",
    "labeled_seq_data = pd.read_csv(labeled_seq_data)\n",
    "\n",
    "pt = np.load(ig_data) #load Ig data\n",
    "\n",
    "labels = labeled_seq_data.iloc[:,1].to_numpy()\n",
    "labels = np.array(labels)\n",
    "\n",
    "sequ = labeled_seq_data.iloc[:,0].to_numpy()\n",
    "sequ = np.array(sequ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3da5cf",
   "metadata": {},
   "source": [
    "### Functions to compute PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b461a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_pca(pc_no):\n",
    "    '''\n",
    "    Computes PCA based on number of principle components provided\n",
    "    '''\n",
    "    pca = PCA(n_components=pc_no)\n",
    "    return pca\n",
    "\n",
    "\n",
    "def pca_transformation(pt, pca, pc1, pc2):\n",
    "    pca_transf = pca.fit_transform(pt.reshape(-1, 20*10))\n",
    "    pca_transf = pca_transf[:, [pc1-1, pc2-1]]\n",
    "    return pca_transf\n",
    "\n",
    "    \n",
    "def graph_pca(pt, labels, sequ, pc1, pc2, pca):\n",
    "    '''\n",
    "    Graphs PCA using IG, label and sequence numpy arrays based on selected principle components and the resulting \n",
    "    PCA transformation from the pca_transform function.\n",
    "    \n",
    "    \n",
    "    Also returns list of rows containing binders/non-binders in separate lists for further use\n",
    "    '''\n",
    "    \n",
    "    pca_transf = pca_transformation(pt, pca, pc1, pc2)\n",
    "    \n",
    "    x_axes = f'PCA{pc1} explained variance: {pca.explained_variance_ratio_[pc1-1]*100:.2f}%'\n",
    "    y_axes = f'PCA{pc2} explained variance: {pca.explained_variance_ratio_[pc2-1]*100:.2f}%'\n",
    "\n",
    "    X = pd.DataFrame(pca_transf, columns = [x_axes, y_axes])\n",
    "    X['Labels'] = pd.DataFrame(labels, columns=['Labels'])['Labels'].apply(lambda x: 'Binder' if x > 0.5 else 'Non Binder')\n",
    "\n",
    "    #plt.figure(figsize = (20,15))\n",
    "\n",
    "    ax = sns.scatterplot(x=x_axes, y=y_axes, data=X, linewidth=0, hue='Labels', s=5)\n",
    "    #ax = sns.scatterplot(x=x_axes, y=y_axes, data=x_nb, linewidth=0, s=5)\n",
    "\n",
    "    # ax.set_axis_off()\n",
    "    ax.set_frame_on(False)\n",
    "    # plt.title('PCA on Integraded Gradients for sequences with implanted signals')\n",
    "    plt.show()\n",
    "    \n",
    "    binders = X.index[X['Labels'] == 'Binder'].tolist()\n",
    "    nonbinders = X.index[X['Labels'] == 'Non Binder'].tolist()\n",
    "    \n",
    "    return binders, nonbinders, X\n",
    "\n",
    "    \n",
    "def compute_and_graph_pca(pt, labels, sequ, pc1, pc2):\n",
    "    '''\n",
    "    Performs all computation steps of the PCA analysis according to the set principle components using the\n",
    "    IG, labels and sequence data\n",
    "    '''\n",
    "        \n",
    "    pca = compute_pca(pc2)\n",
    "    pca_transformat = pca_transformation(pt, pca, pc1, pc2)\n",
    "    \n",
    "    binders_and_nonbinders = graph_pca(pt, labels, sequ, pc1, pc2, pca)\n",
    "    return binders_and_nonbinders\n",
    "\n",
    "\n",
    "\n",
    "def show_all_pc_combos(pt, labels, sequ, pc_no):\n",
    "    '''\n",
    "    Computes and graphs all possible principle component combinations for the set pc number based on IG, label, \n",
    "    sequence.\n",
    "    \n",
    "    Only for quick investigation purposes. Does not return anything\n",
    "    '''\n",
    "    combos = []\n",
    "    pca = compute_pca(pc_no)\n",
    "    \n",
    "    for i in range(1, pc_no+1):\n",
    "        for j in range (1, pc_no+1):\n",
    "            if i !=j and (i,j) not in combos and (j,i) not in combos:\n",
    "                combos.append((i,j))\n",
    "                compute_and_graph_pca(pt, labels, sequ, i, j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae5f75ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pca = compute_pca(10)\n",
    "# bvnb = graph_pca(pt, labels, sequ, 1, 9, pca)\n",
    "# compute_and_graph_pca(pt, labels, sequ, 2, 3)\n",
    "# show_all_pc_combos(pt, labels, sequ, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fc30ec",
   "metadata": {},
   "source": [
    "### 3-D PCA graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e467ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a2a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "017178a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphing_3d(pt, labels, sequ, pc1, pc2, pc3, show_points = None):\n",
    "        \n",
    "    pca = compute_pca(pc3)\n",
    "    pca_transf = pca.fit_transform(pt.reshape(-1, 20*10))\n",
    "    pca_transf = pca_transf[:, [pc1-1, pc2-1, pc3-1]]\n",
    "\n",
    "    x_axes = f'PCA{pc1} expl var: {pca.explained_variance_ratio_[pc1-1]*100:.2f}%'\n",
    "    y_axes = f'PCA{pc2} expl var: {pca.explained_variance_ratio_[pc2-1]*100:.2f}%'\n",
    "    z_axes = f'PCA{pc3} expl var: {pca.explained_variance_ratio_[pc3-1]*100:.2f}%'\n",
    "\n",
    "    X = pd.DataFrame(pca_transf, columns = [x_axes, y_axes, z_axes])\n",
    "    X['Labels'] = pd.DataFrame(labels, columns=['Labels'])['Labels'].apply(lambda x: 'Binder' if x > 0.5 else 'Non Binder')\n",
    "\n",
    "    if show_points == 'binders':\n",
    "        X = X[X['Labels'] == 'Binder']\n",
    "\n",
    "    elif show_points == 'non binders':\n",
    "        X = X[X['Labels'] == 'Binder']\n",
    "\n",
    "    # Create a 3D scatter plot with Plotly\n",
    "    fig = go.Figure(data=[\n",
    "        go.Scatter3d(\n",
    "            x=X[x_axes],\n",
    "            y=X[y_axes],\n",
    "            z=X[z_axes],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=2,\n",
    "                color=X['Labels'].map({'Binder': 'red', 'Non Binder': 'blue'}),\n",
    "                opacity=0.5\n",
    "            )\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # Set labels for each axis\n",
    "    fig.update_layout(scene=dict(xaxis_title=x_axes, yaxis_title=y_axes, zaxis_title=z_axes))\n",
    "    fig.update_layout(width=1200, height=800)\n",
    "\n",
    "    # Show the interactive plot\n",
    "    fig.show()\n",
    "\n",
    "#graphing_3d(pt, labels, sequ, 1, 2, 3, 'binders')\n",
    "    \n",
    "\n",
    "def show_all_pc_combos_3d(pt, labels, sequ, pc_no, show_points = None):\n",
    "    combos = set()\n",
    "    pca = compute_pca(pc_no)\n",
    "    \n",
    "    for i in range(1, pc_no+1):\n",
    "        for j in range (1, pc_no+1):\n",
    "            for k in range (1, pc_no+1):\n",
    "                if i !=j and i != k and j != k: \n",
    "                    current_combo = frozenset({i, j, k}) \n",
    "\n",
    "                    if current_combo not in combos:\n",
    "                        combos.add(current_combo)\n",
    "                        graphing_3d(pt, labels, sequ, i, j, k, 'binders')\n",
    "\n",
    "# show_all_pc_combos_3d(pt, labels, sequ, 10, show_points = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1e4e2",
   "metadata": {},
   "source": [
    "### UMAP Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319e29ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c65f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de318b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3629\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Labels'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14440\\1508306583.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mumap_df_u\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mumap_result_u\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumap_reduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mumap_df_u\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mplot_umap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mumap_result_u\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mumap_df_u\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3505\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3506\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3631\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3632\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3633\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Labels'"
     ]
    }
   ],
   "source": [
    "def flatten_data(unlabeled_data):\n",
    "    flattened_data = unlabeled_data.reshape(unlabeled_data.shape[0], -1)\n",
    "    columns = [f'Feature-{i}' for i in range(1, flattened_data.shape[1] + 1)]\n",
    "    umap_df = pd.DataFrame(flattened_data, columns=columns)\n",
    "    return umap_df\n",
    "\n",
    "# print(flatten_data(pt).head)\n",
    "\n",
    "def flatten_labeled_data(unlabeled_data, labels):\n",
    "    umap_df = flatten_data(unlabeled_data)\n",
    "    umap_df['Labels'] = labels\n",
    "    return umap_df\n",
    "\n",
    "# print(flatten_labeled_data(pt, labels).head)\n",
    "\n",
    "\n",
    "\n",
    "def umap_reduction(umap_df, n_components=2, n_neighbors=15, min_dist=0.1, metric='euclidean', random_state=42):\n",
    "    reducer = umap.UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric=metric,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    umap_result = reducer.fit_transform(umap_df.iloc[:, :-1])  # Assuming the last column is 'Labels'\n",
    "\n",
    "    return pd.DataFrame(umap_result, columns=[f'UMAP-{i+1}' for i in range(n_components)])\n",
    "\n",
    "def plot_umap(umap_result, labels):\n",
    "    umap_df = pd.concat([umap_result, labels], axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x='UMAP-1', y='UMAP-2', hue='Labels', data=umap_df, palette={'Binder': 'red', 'Non Binder': 'blue'}, s=5)\n",
    "    plt.title('UMAP Dimensionality Reduction')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# umap_df_l = flatten_labeled_data(pt, labels)\n",
    "# umap_result_l = umap_reduction(umap_df_l)\n",
    "# plot_umap(umap_result_l, umap_df_l['Labels'])\n",
    "\n",
    "umap_df_u = flatten_data(pt)\n",
    "umap_result_u = umap_reduction(umap_df_u)\n",
    "plot_umap(umap_result_u, umap_df_u['Labels'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9210c24",
   "metadata": {},
   "source": [
    "### Extracting all data points from a given set of vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de05ce",
   "metadata": {},
   "source": [
    "##### Example vertices provided by Robert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "150d9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vertices provided by Robert as examples\n",
    "\n",
    "#-0.0370, 0.0512#rechts unten\n",
    "#-0.1263, 0.1838#links unten\n",
    "#-0.0051, 0.1493#rechts oben\n",
    "#-0.0779, 0.2759#links oben\n",
    "\n",
    "# Mason negative extract\n",
    "vertices_1 = np.asarray([(-0.1263, 0.1838),#links unten\n",
    "                       (-0.0370, 0.0512),#rechts unten\n",
    "                       (-0.0051, 0.1493),#rechts oben\n",
    "                       (-0.0779, 0.2759)])#links oben\n",
    "\n",
    "# Mason spike right down extract\n",
    "vertices_2 = np.asarray([(-0.0735, -0.096),#links unten\n",
    "                       (0.1508, -0.425),#rechts unten\n",
    "                       (0.2074, -0.312),#rechts oben\n",
    "                       (0.0211, -0.04)])#links oben\n",
    "\n",
    "\n",
    "# Mason spike right down up\n",
    "vertices_3 = np.asarray([(0.0484, -0.008),#links unten\n",
    "                       (0.3635, 0.15),#rechts unten\n",
    "                       (0.3703, 0.295),#rechts oben\n",
    "                       (0.0513, 0.121)])#links oben\n",
    "\n",
    "# Brij spike down\n",
    "vertices_4 = np.asarray([(0.0578, -0.045),#links unten\n",
    "                       (0.5968, 0.029),#rechts unten\n",
    "                       (0.5942, 0.098),#rechts oben\n",
    "                       (0.0628, 0.009)])#links oben\n",
    "\n",
    "\n",
    "# Brij spike vertical 1\n",
    "vertices_5 = np.asarray([(-0.0263, 0.089),#links unten\n",
    "                       (-0.0056, 0.093),#rechts unten\n",
    "                       (-0.0784, 0.604),#rechts oben\n",
    "                       (-0.0997, 0.588)])#links oben\n",
    "\n",
    "# Brij spike diagonal\n",
    "vertices_6 = np.asarray([(0.1356, 0.127),#links unten\n",
    "                       (0.2887, 0.264),#rechts unten\n",
    "                       (0.2648, 0.298),#rechts oben\n",
    "                       (0.1256, 0.147)])#links oben\n",
    "\n",
    "vertices_selection = [vertices_1, vertices_2, vertices_3, vertices_4,vertices_5,vertices_6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96636e3c",
   "metadata": {},
   "source": [
    "#### Function to extract the rows/positions of the data points within specified vertices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a2004f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Points Selected: 36    --    Number of Binders: 11    --    Number of Non Binders: 25\n"
     ]
    }
   ],
   "source": [
    "### ploting selection captured in vertices\n",
    "def pca_selection(pt, labels, vertices, pc1, pc2, show_graph = None):\n",
    "    \"\"\"\n",
    "    Returns the positions of the IG sequences entries that fall within the selected vertices\n",
    "    Resulting list can be of all CDRH3 located within the vertices or just the binders/non-binders\n",
    "    Also returns the final complete array for QC purposes\n",
    "    \n",
    "    Final 'show_graph' input can be left blank, or input 'y' or 'yes' to show the resulting graph and selected points\n",
    "    \n",
    "    Returned selection to be used in next functions to extract and convert the sequences to one letter amino acid code\n",
    "    \"\"\"\n",
    "    pca = compute_pca(pc2)\n",
    "    pca_transf = pca_transformation(pt, pca, pc1, pc2)\n",
    "    \n",
    "    path = Path(vertices)\n",
    "    mask = path.contains_points(pca_transf)\n",
    "    \n",
    "    \n",
    "    x_axes = f'PCA{pc1} explained variance: {pca.explained_variance_ratio_[pc1-1]*100:.2f}%'\n",
    "    y_axes = f'PCA{pc2} explained variance: {pca.explained_variance_ratio_[pc2-1]*100:.2f}%'\n",
    "    X = pd.DataFrame(pca_transf, columns = [x_axes, y_axes])\n",
    "    X['Labels'] = pd.DataFrame(labels, columns=['Labels'])['Labels'].apply(lambda x: 'Binder' if x > 0.5 else 'Non Binder')\n",
    "\n",
    "    X['mask'] = mask\n",
    "    \n",
    "    if show_graph == 'y' or show_graph == 'yes': \n",
    "        plt.figure(figsize = (20,15))\n",
    "        ax = sns.scatterplot(x=x_axes, y=y_axes, data=X, linewidth=0, hue='mask', s=5)\n",
    "        ax.set_frame_on(False)\n",
    "        plt.show()\n",
    "        \n",
    "    all_rows = [int(i) for i in X.index[mask]]\n",
    "    binder_rows = [int(i) for i in X.index[mask] if X.loc[i, 'Labels'] == 'Binder']\n",
    "    nonbinder_rows = [int(i) for i in X.index[mask] if X.loc[i, 'Labels'] == 'Non Binder']\n",
    "    \n",
    "    print(f'Number of Points Selected: {len(all_rows)}    --    Number of Binders: {len(binder_rows)}    --    Number of Non Binders: {len(nonbinder_rows)}')\n",
    "    \n",
    "    return all_rows, binder_rows, nonbinder_rows, X \n",
    "  \n",
    "all_rows, selected_binders, selected_nonbinders, xarray = pca_selection(pt, labels, vertices_6, 1, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df145bc6",
   "metadata": {},
   "source": [
    "#### Functions to convert IG 1-hot encoded sequences from selection of row positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e513742e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WSGAGFYEFA', 'WGSRGFYEFA', 'WLGDGFYEFA', 'WRGAGMYEFT', 'WRGAGFYEFA', 'YGAGGMYEFL', 'YSDNGFYEFA', 'YGQISHYEFQ', 'WQGPSFYEFV', 'YRGCSFYEFT', 'YTDCSLYEFK']\n"
     ]
    }
   ],
   "source": [
    "def one_hot_to_one_letter(integrated_gradient_sequence):\n",
    "    '''\n",
    "    Function to convert one-hot encoded sequence to one-letter amino acid sequence string.\n",
    "    '''\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    one_letter_code = \"\"\n",
    "    \n",
    "    for block in integrated_gradient_sequence:\n",
    "        index = next((i for i, value in enumerate(block) if np.any(value != 0)), -1)\n",
    "        if index != -1:\n",
    "            one_letter_code += amino_acids[index]\n",
    "        else:\n",
    "            one_letter_code += \"X\"  # Placeholder for unknown or padding\n",
    "\n",
    "    return one_letter_code\n",
    "\n",
    "\n",
    "\n",
    "def onehot_to_oneletter_selection(selection):\n",
    "    '''\n",
    "    Returns one-letter amino acid sequence of entries, using a list of selected rows as input\n",
    "    \n",
    "    Can be used as an extension of graph_pca, compute_and_graph_pca and pca_selection\n",
    "    '''\n",
    "    selected_cdrh3_seqs = []\n",
    "\n",
    "    for row in selection:\n",
    "        ig_entry = pt[row]\n",
    "        aa_seq = one_hot_to_one_letter(ig_entry)\n",
    "        selected_cdrh3_seqs.append(aa_seq)\n",
    "    \n",
    "    return selected_cdrh3_seqs\n",
    "    \n",
    "print(onehot_to_oneletter_selection(selected_binders))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb52a96",
   "metadata": {},
   "source": [
    "### Function to iterate through list of vertices and export the resulting list into separate txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e3cd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
